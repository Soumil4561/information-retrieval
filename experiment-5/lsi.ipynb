{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latent semantic indexing\n",
    "\n",
    "#first we need to import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to import the dataset 20 newsgroups from sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#now we need to import the dataset\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets do some preprocessing\n",
    "\n",
    "#first we need to import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#now we need to import the stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#now we need to import the lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Soumil\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Soumil\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#removing the \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "corpus = []\n",
    "for i in range(0, len(dataset.data)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset.data[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    #now we need to do stemming\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    #now we need to do lemmatization\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [lm.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    #now we need to join the words\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# #import the tfidf vectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#create a term document matrix using count vectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  abil around among associ app armenian assembl angel anyth altern  \n",
      "Topic 1:  abl absolut atheism area art attempt atheist attitud approach assum  \n",
      "Topic 2:  absolut art attitud assum assembl approach assist area appli anyway  \n",
      "Topic 3:  abus area attack atheism atheist approach art assum anyway appreci  \n",
      "Topic 4:  ac articl assembl armenian arm attempt assist applic appli assert  \n",
      "Topic 5:  acceler armi assert april assist archiv argu art attack armenian  \n",
      "Topic 6:  accept articl attempt assembl assert applic armenian assist apr arm  \n",
      "Topic 7:  access armi april argument armenia archiv assert associ argu atheism  \n",
      "Topic 8:  accord armenia associ assert april archiv armi argument armenian arab  \n",
      "Topic 9:  account art attempt assembl attitud armenian area assum armi archiv  \n",
      "Topic 10:  across area art attitud approach assist assum assembl apr arm  \n",
      "Topic 11:  act assum arm approach attitud art assembl articl assist area  \n",
      "Topic 12:  action atheism area around attempt atheist articl appl ask associ  \n",
      "Topic 13:  activ attack around atheism armenia atheist archiv associ ask april  \n",
      "Topic 14:  actual atheist assum area approach atheism art appl arm anyway  \n",
      "Topic 15:  ad atheism atheist armenia around archiv attack area associ appreci  \n",
      "Topic 16:  adam atheism atheist around attack armi april arm associ armenia  \n",
      "Topic 17:  adapt armenia attack associ april archiv around armi appear answer  \n",
      "Topic 18:  add art area assist approach assum arm appreci anyway apr  \n",
      "Topic 19:  addit assert attempt armenia argument articl arab associ ask attitud  \n",
      "Topic 20:  address attitud assum atheism art argument assert atheist appropri attempt  \n",
      "Topic 21:  administr area assum atheist approach atheism art arm appl appreci  \n",
      "Topic 22:  admit armenia associ attack argument april archiv assert ask armi  \n",
      "Topic 23:  advanc atheist atheism armenia appropri associ attempt appl articl assert  \n",
      "Topic 24:  advantag attack around atheism armenia atheist archiv argu apart appreci  \n",
      "Topic 25:  advic attack approach area anyway appreci assum assembl atheism anyon  \n",
      "Topic 26:  affect art assist attitud arm assum argument apr assert assembl  \n",
      "Topic 27:  age assembl approach attempt area assum anyway armenian appli art  \n",
      "Topic 28:  agenc art attempt assembl argument armenian assert assum appli assist  \n",
      "Topic 29:  agent attack around atheism area atheist approach argu appreci appl  \n",
      "Topic 30:  ago armenia associ archiv april assert argument armi arab answer  \n",
      "Topic 31:  agre attempt assembl assert art articl attitud armenian argument arab  \n",
      "Topic 32:  ah attempt armenia argument assert arab associ ask archiv articl  \n",
      "Topic 33:  aid attempt art attitud assembl area assum argument approach appli  \n",
      "Topic 34:  air attitud assum approach assembl anyway appli art armenian arm  \n",
      "Topic 35:  al around armenia attack atheist atheism archiv associ april appear  \n",
      "Topic 36:  algorithm attempt assembl area assum art articl approach appl attitud  \n",
      "Topic 37:  allow armenia attempt assert argument ask arab associ archiv apr  \n",
      "Topic 38:  almost articl attempt applic appl around assembl armenian arm andrew  \n",
      "Topic 39:  alon attitud approach assum area atheism assembl anyon anyway art  \n",
      "Topic 40:  along attack armenia attitud atheist around ask apr assert area  \n",
      "Topic 41:  alreadi assert armenia argument attempt ask articl arab armi armenian  \n",
      "Topic 42:  also associ assist argument apr armenia april arab appear arm  \n",
      "Topic 43:  alt atheism attempt area art assembl attitud ask assert approach  \n",
      "Topic 44:  altern argument attitud assum armenia associ atheist armi assert armenian  \n",
      "Topic 45:  although assum art assembl attempt armenian appli assist attitud applic  \n",
      "Topic 46:  alway amend armenia atheism although associ answer argument atheist archiv  \n",
      "Topic 47:  amend armenia archiv associ around attack answer april articl app  \n",
      "Topic 48:  america attack armenia april associ among approach attitud analysi announc  \n",
      "Topic 49:  american america armenian articl attempt armi assembl argu applic archiv  \n",
      "Topic 50:  among amount attitud associ april armenia arm archiv ask appear  \n",
      "Topic 51:  amount armenia assert argument associ arab anim ask answer april  \n",
      "Topic 52:  analysi assembl attempt assert attack art armenian anyway appli angel  \n",
      "Topic 53:  andrew articl around assembl applic attack approach anoth apart arm  \n",
      "Topic 54:  angel attitud art attempt assembl area assum approach anyon armenian  \n",
      "Topic 55:  anim angel armenia assert answer associ archiv articl argument april  \n",
      "Topic 56:  announc attack armenia around apart april archiv angel anoth anyth  \n",
      "Topic 57:  anonym atheism atheist around approach area appreci arm answer apart  \n",
      "Topic 58:  anoth answer articl atheist applic attack apart argu arm approach  \n",
      "Topic 59:  answer armenia around archiv associ appear april app apart arab  \n",
      "Topic 60:  anti anybodi attempt armenian argument anyon armi answer assum anyway  \n",
      "Topic 61:  anybodi argument anyway assert argu art anyon appli armenian attempt  \n",
      "Topic 62:  anyon anyway anyth approach area art atheism appli assum appl  \n",
      "Topic 63:  anyth anyway area attack art approach assist april apr atheism  \n",
      "Topic 64:  anyway approach appli apart appreci area assum art attack answer  \n",
      "Topic 65:  apart app attack around argu archiv appear anyon armenia april  \n",
      "Topic 66:  app appear anyway appar archiv armenia anyon april associ around  \n",
      "Topic 67:  appar appear april anyon argument appropri armenia armi attitud arm  \n",
      "Topic 68:  appear armenia april anyway archiv associ anyon around arab ask  \n",
      "Topic 69:  appl area approach applic atheism appear appreci attempt assum appropri  \n",
      "Topic 70:  appli approach appear applic art assembl assum armenian appreci app  \n",
      "Topic 71:  applic appropri articl approach assembl attempt armenian anyth argu appear  \n",
      "Topic 72:  appreci approach area atheism applic attack atheist assum armi argu  \n",
      "Topic 73:  approach area assum atheist atheism attack appear attitud art app  \n",
      "Topic 74:  appropri approach armenia argument assum attitud atheist armenian armi arm  \n",
      "Topic 75:  apr area arab appropri appreci applic art around arm approach  \n",
      "Topic 76:  april archiv arab approach armenia appreci associ arm around armi  \n",
      "Topic 77:  arab archiv armenia approach argument assert appreci ask around articl  \n",
      "Topic 78:  archiv armenia around apr approach armi associ atheism appropri appli  \n",
      "Topic 79:  area archiv arab april art atheism atheist appropri attempt attack  \n",
      "Topic 80:  argu area arab armi argument apr archiv attack approach april  \n",
      "Topic 81:  argument area armenia armenian assert armi approach art attempt attitud  \n",
      "Topic 82:  arm argument assist archiv assum attitud articl art armenian armi  \n",
      "Topic 83:  armenia arm area around ask armi associ approach attack assert  \n",
      "Topic 84:  armenian articl assembl art attempt area armi assert assum april  \n",
      "Topic 85:  armi area apr assum attitud arab approach atheism applic andrew  \n",
      "Topic 86:  around armi armenian ask argument attack atheism atheist articl associ  \n",
      "Topic 87:  art around assembl armenia armi assist attempt assert attitud articl  \n",
      "Topic 88:  articl armi assert assembl attempt ask assist approach archiv april  \n",
      "Topic 89:  ask assert atheism associ armi armenian assembl attitud argu art  \n",
      "Topic 90:  assembl assert attempt armi attitud armenia around assist assum argu  \n",
      "Topic 91:  assert assist attempt area around attitud associ approach arm armi  \n",
      "Topic 92:  assist attitud armenia attack armi ask armenian archiv appropri approach  \n",
      "Topic 93:  associ assum atheist assembl atheism art attitud assist articl area  \n",
      "Topic 94:  attack associ art area arm approach armenian armi appropri argument  \n",
      "Topic 95:  atheism atheist assist attack assembl assert armenian arm argument articl  \n",
      "Topic 96:  attempt atheism around armenia area approach apr april argu archiv  \n",
      "Topic 97:  atheist attitud assert assist assembl armi argu art april archiv  \n",
      "Topic 98:  attempt attack atheist associ assist assum arm armi around april  \n",
      "Topic 99:  attitud attempt attack around articl argu archiv atheism armenia appreci  \n"
     ]
    }
   ],
   "source": [
    "#apply svd on the term document matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, random_state=0)\n",
    "X = svd.fit_transform(X)\n",
    "\n",
    "#find the singular vectors corresponding to the top singular values\n",
    "U = svd.components_\n",
    "\n",
    "#find the singular values\n",
    "S = svd.singular_values_\n",
    "\n",
    "#find the variance explained by the top singular values\n",
    "explained_variance = svd.explained_variance_ratio_\n",
    "\n",
    "#now we need to find the top 10 words in each topic\n",
    "terms = cv.get_feature_names_out()\n",
    "\n",
    "for i, comp in enumerate(U):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \", end=\" \")\n",
    "    #print in one line\n",
    "    for t in sorted_terms:\n",
    "        print(t[0], end=\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms in the first topic:\n",
      "['abil', 'around', 'among', 'associ', 'app', 'armenian', 'assembl', 'angel', 'anyth', 'altern']\n"
     ]
    }
   ],
   "source": [
    "singular_values = svd.singular_values_\n",
    "singular_vectors = svd.components_\n",
    "\n",
    "# Assuming you want to analyze the first topic\n",
    "first_topic = singular_vectors[0]\n",
    "\n",
    "# Get the indices of the terms with the highest weightings in the first topic\n",
    "top_indices = first_topic.argsort()[::-1][:10]  # Adjust the number 10 as needed\n",
    "\n",
    "# Get the feature names (terms) from the CountVectorizer\n",
    "feature_names = cv.get_feature_names_out()\n",
    "\n",
    "# Get the terms with the highest weightings in the first topic\n",
    "top_terms = [feature_names[i] for i in top_indices]\n",
    "\n",
    "print(\"Top terms in the first topic:\")\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1500 features, but TruncatedSVD is expecting 100 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\CodeSpace\\IR\\lsi\\lsi.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CodeSpace/IR/lsi/lsi.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m query_vec \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mtransform([query])\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CodeSpace/IR/lsi/lsi.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Query reduction\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/CodeSpace/IR/lsi/lsi.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m query_vec_reduced \u001b[39m=\u001b[39m svd\u001b[39m.\u001b[39;49mtransform(query_vec)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CodeSpace/IR/lsi/lsi.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Calculate the cosine similarities\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CodeSpace/IR/lsi/lsi.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n",
      "File \u001b[1;32mc:\\Users\\Soumil Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Soumil Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:292\u001b[0m, in \u001b[0;36mTruncatedSVD.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Perform dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39m    Reduced version of X. This will always be a dense array.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 292\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    293\u001b[0m \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_\u001b[39m.\u001b[39mT)\n",
      "File \u001b[1;32mc:\\Users\\Soumil Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 626\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    628\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Soumil Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 415\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1500 features, but TruncatedSVD is expecting 100 features as input."
     ]
    }
   ],
   "source": [
    "# Query preprocessing\n",
    "query = \"Best Graphics card for gaming in 2023\" \n",
    "query = re.sub('[^a-zA-Z]', ' ', query)\n",
    "query = query.lower()\n",
    "query = query.split()\n",
    "\n",
    "#now we need to do stemming\n",
    "query = [ps.stem(word) for word in query if not word in set(stopwords.words('english'))]\n",
    "\n",
    "#now we need to do lemmatization\n",
    "query = [lm.lemmatize(word) for word in query if not word in set(stopwords.words('english'))]\n",
    "\n",
    "#now we need to join the words\n",
    "query = ' '.join(query)\n",
    "\n",
    "# Query vectorization\n",
    "query_vec = cv.transform([query]).toarray()\n",
    "\n",
    "# Query reduction\n",
    "query_vec_reduced = svd.transform(query_vec)\n",
    "\n",
    "# Calculate the cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarities = cosine_similarity(query_vec_reduced, X)\n",
    "\n",
    "# Get the top 5 most similar documents\n",
    "most_similar_doc_indices = cosine_similarities.argsort()[0][::-1][:5]  # Adjust the number 5 as needed\n",
    "\n",
    "# Print the most similar documents\n",
    "print(\"Most similar documents:\")\n",
    "with open(\"documents.txt\", \"w\") as f:\n",
    "    for i in most_similar_doc_indices:\n",
    "        f.write(\"Document \"+str(i+1)+\": \\n\")\n",
    "        f.write(dataset.data[i])\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"----------------------------------------------------------------\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the top n documents similar to the query\n",
    "n = 5\n",
    "top_n = np.argsort(similarities, axis=1)[:,-n:]\n",
    "\n",
    "#write the top n documents to a file\n",
    "with open(\"documents.txt\", \"w\") as f:\n",
    "    for i in range(len(top_n[0])):\n",
    "        f.write(\"Document \"+str(i+1)+\": \\n\")\n",
    "        f.write(dataset.data[top_n[0][i]])\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"----------------------------------------------------------------\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soumil Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Clusters: 20\n",
      "Silhouette Score: 0.9537381349565673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, silhouette_score\n",
    "\n",
    "# Cluster the documents\n",
    "n_clusters = 20  # Adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "document_clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate clustering evaluation metrics\n",
    "# You may need the true labels for the 20 newsgroups dataset for this part\n",
    "true_labels = dataset.target\n",
    "\n",
    "purity = purity_score(true_labels, document_clusters)\n",
    "nmi = normalized_mutual_info_score(true_labels, document_clusters)\n",
    "silhouette = silhouette_score(X, document_clusters)\n",
    "\n",
    "print(f'\\nNumber of Clusters: {n_clusters}')\n",
    "print(f'Silhouette Score: {silhouette}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
